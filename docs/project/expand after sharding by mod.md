# 取模分表后扩容方案思考

## 背景
在面向用户的业务场景中，常常需要按用户维度进行分表来处理数据量太多影响查询效率的问题，
当数据量再次达到瓶颈时，需要再次处理，这里常常有两种方案可以处理：
- 老数据封存

之前在分表的实施过程中，dba给的方案也包括老数据封存，大概就是封存两年以前的数据，
所以当时分表30张，正好可以支撑两年的量。
这里自己的想法是，如果是完全没有价值，完全不要也可以；如果有用，常常是一些统计数据，
那么可以维护一些统计表。

- 再度扩容

按用户取模分表的坏处就是扩容时数据处理会很头痛，初始分表时，通过maxwell等工具同步数据，
上线时，先停服，等数据完全同步后再启动服务，但这显然是不好的，影响用户体验和线上流量，
所以需要停服以外更好的方案。

## 方案思考
其实redis的hash槽和memberCache的一致性hash，都做了这个处理，还有就是ConcurrentHashMap的扩容。
- 历史数据可以提前进行扩容，通过dba或者程序
- 热点数据需要特殊处理，虽然dba也可以一直做同步
- 对于查询，借鉴redis，查询老表，查询不到再查询一次新表
- 对于新增和修改或者删除

了减少复杂度，不做特殊处理，可以考虑把需要迁移的数据再分片，假如1000份，这表示需要进行1000次子扩容操作，
每当进行扩容时，通过redis或者数据库将该分片置为迁移中，然后进行分片，当分片完成修改为分片完成，
当有新建，编辑时，如果在分片，提示系统升级，这里有问题就是：
老的事务未提交的可能不能及时处理，需要等老的事务提交完成, 
这样和直接停该分片功能也差别不大了，毕竟通过工具（Maxwell等）的同步也很快，
也不需要像上面一样等待事务提交完成才能处理，影响时间也更短

